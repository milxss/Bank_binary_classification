{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "binary_classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SxA3pI9rbNT"
      },
      "source": [
        "# we want to train a model that will predict if client will leave a bank or not"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVsenqEHoybt"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ha5i3BMMo3k-"
      },
      "source": [
        "df = pd.read_csv(\"/content/Churn_Modelling.csv\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "2NtOhuwkpAN1",
        "outputId": "de1639e3-5c5c-4e60-e231-353330d1143b"
      },
      "source": [
        "df"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RowNumber</th>\n",
              "      <th>CustomerId</th>\n",
              "      <th>Surname</th>\n",
              "      <th>CreditScore</th>\n",
              "      <th>Geography</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Age</th>\n",
              "      <th>Tenure</th>\n",
              "      <th>Balance</th>\n",
              "      <th>NumOfProducts</th>\n",
              "      <th>HasCrCard</th>\n",
              "      <th>IsActiveMember</th>\n",
              "      <th>EstimatedSalary</th>\n",
              "      <th>Exited</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>15634602</td>\n",
              "      <td>Hargrave</td>\n",
              "      <td>619</td>\n",
              "      <td>France</td>\n",
              "      <td>Female</td>\n",
              "      <td>42</td>\n",
              "      <td>2</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>101348.88</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>15647311</td>\n",
              "      <td>Hill</td>\n",
              "      <td>608</td>\n",
              "      <td>Spain</td>\n",
              "      <td>Female</td>\n",
              "      <td>41</td>\n",
              "      <td>1</td>\n",
              "      <td>83807.86</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>112542.58</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>15619304</td>\n",
              "      <td>Onio</td>\n",
              "      <td>502</td>\n",
              "      <td>France</td>\n",
              "      <td>Female</td>\n",
              "      <td>42</td>\n",
              "      <td>8</td>\n",
              "      <td>159660.80</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>113931.57</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>15701354</td>\n",
              "      <td>Boni</td>\n",
              "      <td>699</td>\n",
              "      <td>France</td>\n",
              "      <td>Female</td>\n",
              "      <td>39</td>\n",
              "      <td>1</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>93826.63</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>15737888</td>\n",
              "      <td>Mitchell</td>\n",
              "      <td>850</td>\n",
              "      <td>Spain</td>\n",
              "      <td>Female</td>\n",
              "      <td>43</td>\n",
              "      <td>2</td>\n",
              "      <td>125510.82</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>79084.10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>9996</td>\n",
              "      <td>15606229</td>\n",
              "      <td>Obijiaku</td>\n",
              "      <td>771</td>\n",
              "      <td>France</td>\n",
              "      <td>Male</td>\n",
              "      <td>39</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>96270.64</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>9997</td>\n",
              "      <td>15569892</td>\n",
              "      <td>Johnstone</td>\n",
              "      <td>516</td>\n",
              "      <td>France</td>\n",
              "      <td>Male</td>\n",
              "      <td>35</td>\n",
              "      <td>10</td>\n",
              "      <td>57369.61</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>101699.77</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>9998</td>\n",
              "      <td>15584532</td>\n",
              "      <td>Liu</td>\n",
              "      <td>709</td>\n",
              "      <td>France</td>\n",
              "      <td>Female</td>\n",
              "      <td>36</td>\n",
              "      <td>7</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>42085.58</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>9999</td>\n",
              "      <td>15682355</td>\n",
              "      <td>Sabbatini</td>\n",
              "      <td>772</td>\n",
              "      <td>Germany</td>\n",
              "      <td>Male</td>\n",
              "      <td>42</td>\n",
              "      <td>3</td>\n",
              "      <td>75075.31</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>92888.52</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>10000</td>\n",
              "      <td>15628319</td>\n",
              "      <td>Walker</td>\n",
              "      <td>792</td>\n",
              "      <td>France</td>\n",
              "      <td>Female</td>\n",
              "      <td>28</td>\n",
              "      <td>4</td>\n",
              "      <td>130142.79</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>38190.78</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 14 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      RowNumber  CustomerId    Surname  ...  IsActiveMember EstimatedSalary Exited\n",
              "0             1    15634602   Hargrave  ...               1       101348.88      1\n",
              "1             2    15647311       Hill  ...               1       112542.58      0\n",
              "2             3    15619304       Onio  ...               0       113931.57      1\n",
              "3             4    15701354       Boni  ...               0        93826.63      0\n",
              "4             5    15737888   Mitchell  ...               1        79084.10      0\n",
              "...         ...         ...        ...  ...             ...             ...    ...\n",
              "9995       9996    15606229   Obijiaku  ...               0        96270.64      0\n",
              "9996       9997    15569892  Johnstone  ...               1       101699.77      0\n",
              "9997       9998    15584532        Liu  ...               1        42085.58      1\n",
              "9998       9999    15682355  Sabbatini  ...               0        92888.52      1\n",
              "9999      10000    15628319     Walker  ...               0        38190.78      0\n",
              "\n",
              "[10000 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbxUXL6LpDal"
      },
      "source": [
        "y = df['Exited']"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeA9X3DQpP9u"
      },
      "source": [
        "X = df.iloc[:, 3: 13].values"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcRjxxVmrHyP",
        "outputId": "0d80492f-a216-4c04-b5b8-3056da3b6959"
      },
      "source": [
        "X"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[619, 'France', 'Female', ..., 1, 1, 101348.88],\n",
              "       [608, 'Spain', 'Female', ..., 0, 1, 112542.58],\n",
              "       [502, 'France', 'Female', ..., 1, 0, 113931.57],\n",
              "       ...,\n",
              "       [709, 'France', 'Female', ..., 0, 1, 42085.58],\n",
              "       [772, 'Germany', 'Male', ..., 1, 0, 92888.52],\n",
              "       [792, 'France', 'Female', ..., 1, 0, 38190.78]], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxoMqSgxrK6s",
        "outputId": "a0948663-5117-44d4-8635-f03150e50b7e"
      },
      "source": [
        "y"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       1\n",
              "1       0\n",
              "2       1\n",
              "3       0\n",
              "4       0\n",
              "       ..\n",
              "9995    0\n",
              "9996    0\n",
              "9997    1\n",
              "9998    1\n",
              "9999    0\n",
              "Name: Exited, Length: 10000, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqMZlY4Vr17F"
      },
      "source": [
        "#pd.get_dummies(obj_df, columns=[\"Gender\", \"Geography\"], prefix=[\"body\", \"drive\"]).head()\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KI-u9SyitJAk"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelencoder_X_1 = LabelEncoder() #initiate an object\n",
        "X[:,1] = labelencoder_X_1.fit_transform(X[:,1]) \n",
        "\n",
        "labelencoder_X_2 = LabelEncoder()\n",
        "X[:,2] = labelencoder_X_2.fit_transform(X[:,2]) "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "dFuMASLztJ9Y",
        "outputId": "aa9104e2-4d23-48f2-9871-a78fa2517504"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "import numpy as np\n",
        "ct = ColumnTransformer(#'encoder' is the name of the column transformer)\n",
        "  [('encoder', OneHotEncoder(), [1])],  #the column to work on is 1, but can be 2 and 3\n",
        "  remainder = 'passthrough')    #leave other columns untouched\n",
        "\n",
        "X = np.array(ct.fit_transform(X), dtype = np.float)\n",
        "df = pd.DataFrame(X)\n",
        "df"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>619.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>101348.88</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>608.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>41.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>83807.86</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>112542.58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>502.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>159660.80</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>113931.57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>699.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>93826.63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>850.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>43.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>125510.82</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>79084.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>771.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>96270.64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>516.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>57369.61</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>101699.77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>709.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>42085.58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>772.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>75075.31</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>92888.52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>792.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>130142.79</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>38190.78</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       0    1    2      3    4   ...         7    8    9    10         11\n",
              "0     1.0  0.0  0.0  619.0  0.0  ...       0.00  1.0  1.0  1.0  101348.88\n",
              "1     0.0  0.0  1.0  608.0  0.0  ...   83807.86  1.0  0.0  1.0  112542.58\n",
              "2     1.0  0.0  0.0  502.0  0.0  ...  159660.80  3.0  1.0  0.0  113931.57\n",
              "3     1.0  0.0  0.0  699.0  0.0  ...       0.00  2.0  0.0  0.0   93826.63\n",
              "4     0.0  0.0  1.0  850.0  0.0  ...  125510.82  1.0  1.0  1.0   79084.10\n",
              "...   ...  ...  ...    ...  ...  ...        ...  ...  ...  ...        ...\n",
              "9995  1.0  0.0  0.0  771.0  1.0  ...       0.00  2.0  1.0  0.0   96270.64\n",
              "9996  1.0  0.0  0.0  516.0  1.0  ...   57369.61  1.0  1.0  1.0  101699.77\n",
              "9997  1.0  0.0  0.0  709.0  0.0  ...       0.00  1.0  0.0  1.0   42085.58\n",
              "9998  0.0  1.0  0.0  772.0  1.0  ...   75075.31  2.0  1.0  0.0   92888.52\n",
              "9999  1.0  0.0  0.0  792.0  0.0  ...  130142.79  1.0  1.0  0.0   38190.78\n",
              "\n",
              "[10000 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "3Zak_U_QuAYN",
        "outputId": "e468a986-8826-4005-e24f-17b6e5b139ee"
      },
      "source": [
        "#we remove the first column to avoid the dummy data trap\n",
        "X = X[:,1:]\n",
        "df = pd.DataFrame(X)\n",
        "df"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>619.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>101348.88</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>608.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>41.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>83807.86</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>112542.58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>502.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>159660.80</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>113931.57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>699.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>93826.63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>850.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>43.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>125510.82</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>79084.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>771.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>96270.64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>516.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>57369.61</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>101699.77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>709.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>42085.58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>772.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>75075.31</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>92888.52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>792.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>130142.79</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>38190.78</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       0    1      2    3     4     5          6    7    8    9          10\n",
              "0     0.0  0.0  619.0  0.0  42.0   2.0       0.00  1.0  1.0  1.0  101348.88\n",
              "1     0.0  1.0  608.0  0.0  41.0   1.0   83807.86  1.0  0.0  1.0  112542.58\n",
              "2     0.0  0.0  502.0  0.0  42.0   8.0  159660.80  3.0  1.0  0.0  113931.57\n",
              "3     0.0  0.0  699.0  0.0  39.0   1.0       0.00  2.0  0.0  0.0   93826.63\n",
              "4     0.0  1.0  850.0  0.0  43.0   2.0  125510.82  1.0  1.0  1.0   79084.10\n",
              "...   ...  ...    ...  ...   ...   ...        ...  ...  ...  ...        ...\n",
              "9995  0.0  0.0  771.0  1.0  39.0   5.0       0.00  2.0  1.0  0.0   96270.64\n",
              "9996  0.0  0.0  516.0  1.0  35.0  10.0   57369.61  1.0  1.0  1.0  101699.77\n",
              "9997  0.0  0.0  709.0  0.0  36.0   7.0       0.00  1.0  0.0  1.0   42085.58\n",
              "9998  1.0  0.0  772.0  1.0  42.0   3.0   75075.31  2.0  1.0  0.0   92888.52\n",
              "9999  0.0  0.0  792.0  0.0  28.0   4.0  130142.79  1.0  1.0  0.0   38190.78\n",
              "\n",
              "[10000 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "eI3I9V8bwyIv",
        "outputId": "ed57767b-806b-4d14-cdcf-7e94781f5dda"
      },
      "source": [
        "df"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>619.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>101348.88</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>608.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>41.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>83807.86</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>112542.58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>502.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>159660.80</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>113931.57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>699.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>93826.63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>850.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>43.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>125510.82</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>79084.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>771.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>96270.64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>516.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>57369.61</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>101699.77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>709.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>42085.58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>772.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>75075.31</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>92888.52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>792.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>130142.79</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>38190.78</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       0    1      2    3     4     5          6    7    8    9          10\n",
              "0     0.0  0.0  619.0  0.0  42.0   2.0       0.00  1.0  1.0  1.0  101348.88\n",
              "1     0.0  1.0  608.0  0.0  41.0   1.0   83807.86  1.0  0.0  1.0  112542.58\n",
              "2     0.0  0.0  502.0  0.0  42.0   8.0  159660.80  3.0  1.0  0.0  113931.57\n",
              "3     0.0  0.0  699.0  0.0  39.0   1.0       0.00  2.0  0.0  0.0   93826.63\n",
              "4     0.0  1.0  850.0  0.0  43.0   2.0  125510.82  1.0  1.0  1.0   79084.10\n",
              "...   ...  ...    ...  ...   ...   ...        ...  ...  ...  ...        ...\n",
              "9995  0.0  0.0  771.0  1.0  39.0   5.0       0.00  2.0  1.0  0.0   96270.64\n",
              "9996  0.0  0.0  516.0  1.0  35.0  10.0   57369.61  1.0  1.0  1.0  101699.77\n",
              "9997  0.0  0.0  709.0  0.0  36.0   7.0       0.00  1.0  0.0  1.0   42085.58\n",
              "9998  1.0  0.0  772.0  1.0  42.0   3.0   75075.31  2.0  1.0  0.0   92888.52\n",
              "9999  0.0  0.0  792.0  0.0  28.0   4.0  130142.79  1.0  1.0  0.0   38190.78\n",
              "\n",
              "[10000 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJft7kLFH84x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "c680c509-0530-4d15-c351-4ceb17dba7f5"
      },
      "source": [
        "'''#one-hot encoding:\n",
        "#way 2, using df\n",
        "x_df = dataset.iloc[:,3:13]\n",
        "x_df = pd.concat[(x_df, pd.get_dummies(x_df['Geography'], prefix = 'country', drop_first = True)], axos=1) #drops forst column\n",
        "#axis=1 means to concatinate along the columns(put one column besides another)\n",
        "x_df.drop(['Geography'], axis=1, implace=True) #get rid of original geography column \n",
        "x_df'''"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"#one-hot encoding:\\n#way 2, using df\\nx_df = dataset.iloc[:,3:13]\\nx_df = pd.concat[(x_df, pd.get_dummies(x_df['Geography'], prefix = 'country', drop_first = True)], axos=1) #drops forst column\\n#axis=1 means to concatinate along the columns(put one column besides another)\\nx_df.drop(['Geography'], axis=1, implace=True) #get rid of original geography column \\nx_df\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dpf1TqVmJ7vj"
      },
      "source": [
        "#split data into training and test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "eTVT3SRMKirW",
        "outputId": "cb13f3ff-8848-4dfd-b77d-95b695122765"
      },
      "source": [
        "#standardize the data \n",
        "'''mean = X_train.mean(axis=0)\n",
        "std = X_train.std(axis=0)\n",
        "X_train = (X_train - mean) / std\n",
        "X_test = (X_test - mean) / std'''\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "df = pd.DataFrame(X_train)\n",
        "df"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.734363</td>\n",
              "      <td>-0.573886</td>\n",
              "      <td>1.789744</td>\n",
              "      <td>0.912557</td>\n",
              "      <td>0.943942</td>\n",
              "      <td>1.042715</td>\n",
              "      <td>0.909825</td>\n",
              "      <td>-0.918722</td>\n",
              "      <td>0.640072</td>\n",
              "      <td>0.975305</td>\n",
              "      <td>-0.560873</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.576580</td>\n",
              "      <td>1.742506</td>\n",
              "      <td>0.260405</td>\n",
              "      <td>-1.095822</td>\n",
              "      <td>2.359325</td>\n",
              "      <td>-0.350532</td>\n",
              "      <td>0.649509</td>\n",
              "      <td>-0.918722</td>\n",
              "      <td>0.640072</td>\n",
              "      <td>0.975305</td>\n",
              "      <td>-0.156221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.576580</td>\n",
              "      <td>1.742506</td>\n",
              "      <td>-0.225264</td>\n",
              "      <td>0.912557</td>\n",
              "      <td>-0.471440</td>\n",
              "      <td>0.346091</td>\n",
              "      <td>-1.220910</td>\n",
              "      <td>0.799318</td>\n",
              "      <td>0.640072</td>\n",
              "      <td>-1.025320</td>\n",
              "      <td>1.569218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.576580</td>\n",
              "      <td>1.742506</td>\n",
              "      <td>0.394739</td>\n",
              "      <td>-1.095822</td>\n",
              "      <td>-0.754517</td>\n",
              "      <td>-0.698844</td>\n",
              "      <td>1.014868</td>\n",
              "      <td>-0.918722</td>\n",
              "      <td>-1.562325</td>\n",
              "      <td>0.975305</td>\n",
              "      <td>0.357143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.576580</td>\n",
              "      <td>-0.573886</td>\n",
              "      <td>1.407409</td>\n",
              "      <td>0.912557</td>\n",
              "      <td>1.132660</td>\n",
              "      <td>-1.743780</td>\n",
              "      <td>-0.291139</td>\n",
              "      <td>-0.918722</td>\n",
              "      <td>-1.562325</td>\n",
              "      <td>0.975305</td>\n",
              "      <td>0.737628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7995</th>\n",
              "      <td>-0.576580</td>\n",
              "      <td>-0.573886</td>\n",
              "      <td>0.270738</td>\n",
              "      <td>0.912557</td>\n",
              "      <td>-0.848876</td>\n",
              "      <td>-1.395468</td>\n",
              "      <td>0.028652</td>\n",
              "      <td>-0.918722</td>\n",
              "      <td>-1.562325</td>\n",
              "      <td>0.975305</td>\n",
              "      <td>1.288797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7996</th>\n",
              "      <td>-0.576580</td>\n",
              "      <td>1.742506</td>\n",
              "      <td>-0.535265</td>\n",
              "      <td>0.912557</td>\n",
              "      <td>-0.565799</td>\n",
              "      <td>-0.350532</td>\n",
              "      <td>-0.394232</td>\n",
              "      <td>-0.918722</td>\n",
              "      <td>0.640072</td>\n",
              "      <td>-1.025320</td>\n",
              "      <td>0.199888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7997</th>\n",
              "      <td>1.734363</td>\n",
              "      <td>-0.573886</td>\n",
              "      <td>-1.971603</td>\n",
              "      <td>-1.095822</td>\n",
              "      <td>0.094713</td>\n",
              "      <td>0.346091</td>\n",
              "      <td>0.690343</td>\n",
              "      <td>0.799318</td>\n",
              "      <td>0.640072</td>\n",
              "      <td>-1.025320</td>\n",
              "      <td>-0.149070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7998</th>\n",
              "      <td>-0.576580</td>\n",
              "      <td>-0.573886</td>\n",
              "      <td>0.601406</td>\n",
              "      <td>-1.095822</td>\n",
              "      <td>-0.471440</td>\n",
              "      <td>-1.047156</td>\n",
              "      <td>0.564993</td>\n",
              "      <td>-0.918722</td>\n",
              "      <td>0.640072</td>\n",
              "      <td>-1.025320</td>\n",
              "      <td>-0.749696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7999</th>\n",
              "      <td>1.734363</td>\n",
              "      <td>-0.573886</td>\n",
              "      <td>0.219071</td>\n",
              "      <td>0.912557</td>\n",
              "      <td>0.094713</td>\n",
              "      <td>1.739339</td>\n",
              "      <td>0.426029</td>\n",
              "      <td>-0.918722</td>\n",
              "      <td>0.640072</td>\n",
              "      <td>-1.025320</td>\n",
              "      <td>-1.719296</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8000 rows × 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0         1         2   ...        8         9         10\n",
              "0     1.734363 -0.573886  1.789744  ...  0.640072  0.975305 -0.560873\n",
              "1    -0.576580  1.742506  0.260405  ...  0.640072  0.975305 -0.156221\n",
              "2    -0.576580  1.742506 -0.225264  ...  0.640072 -1.025320  1.569218\n",
              "3    -0.576580  1.742506  0.394739  ... -1.562325  0.975305  0.357143\n",
              "4    -0.576580 -0.573886  1.407409  ... -1.562325  0.975305  0.737628\n",
              "...        ...       ...       ...  ...       ...       ...       ...\n",
              "7995 -0.576580 -0.573886  0.270738  ... -1.562325  0.975305  1.288797\n",
              "7996 -0.576580  1.742506 -0.535265  ...  0.640072 -1.025320  0.199888\n",
              "7997  1.734363 -0.573886 -1.971603  ...  0.640072 -1.025320 -0.149070\n",
              "7998 -0.576580 -0.573886  0.601406  ...  0.640072 -1.025320 -0.749696\n",
              "7999  1.734363 -0.573886  0.219071  ...  0.640072 -1.025320 -1.719296\n",
              "\n",
              "[8000 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KATiBONmLYdk"
      },
      "source": [
        "import tensorflow\n",
        "import keras \n",
        "from tensorflow.keras.models import Sequential"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2gIsUQ2L30V"
      },
      "source": [
        "model = Sequential()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fMgCQDdMYvm"
      },
      "source": [
        "from tensorflow.keras.layers import Dense"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pew3lcitMauk"
      },
      "source": [
        "model.add(Dense(units=6, input_shape=(11, ), kernel_initializer = 'uniform', activation = 'relu', name='dense_layer1'))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkOwbi83Mm6z"
      },
      "source": [
        "model.add(Dense(units=6, input_shape=(11, ),kernel_initializer = 'uniform',activation = 'relu', name='dense_layer2'))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zrcEhrUMqRk"
      },
      "source": [
        "model.add(Dense(1, activation = 'sigmoid', name = 'dense_output'))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQOcRA_GMz8M"
      },
      "source": [
        "model.compile(loss='binary_crossentropy',   #mean-squere error\n",
        "              optimizer= 'adam',\n",
        "              metrics = ['accuracy']) #we want to use adam optimizer"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DB7ZzicHNUfF",
        "outputId": "4298b7f6-667d-447c-b6cd-6ef6fdf4c635"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_layer1 (Dense)         (None, 6)                 72        \n",
            "_________________________________________________________________\n",
            "dense_layer2 (Dense)         (None, 6)                 42        \n",
            "_________________________________________________________________\n",
            "dense_output (Dense)         (None, 1)                 7         \n",
            "=================================================================\n",
            "Total params: 121\n",
            "Trainable params: 121\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiQEqcpzNpvp",
        "outputId": "49512a64-2493-4efa-9106-9a4b9d28dcff"
      },
      "source": [
        "#training\n",
        "history = model.fit(X_train, y_train, batch_size =10, epochs=100) #verbose=0)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "800/800 [==============================] - 2s 1ms/step - loss: 0.5575 - accuracy: 0.7860\n",
            "Epoch 2/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.4214 - accuracy: 0.8198\n",
            "Epoch 3/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.4082 - accuracy: 0.8301\n",
            "Epoch 4/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.4078 - accuracy: 0.8338\n",
            "Epoch 5/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.4139 - accuracy: 0.8255\n",
            "Epoch 6/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3909 - accuracy: 0.8401\n",
            "Epoch 7/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.4037 - accuracy: 0.8344\n",
            "Epoch 8/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3896 - accuracy: 0.8409\n",
            "Epoch 9/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.4008 - accuracy: 0.8330\n",
            "Epoch 10/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.4059 - accuracy: 0.8286\n",
            "Epoch 11/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3961 - accuracy: 0.8410\n",
            "Epoch 12/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3959 - accuracy: 0.8362\n",
            "Epoch 13/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3932 - accuracy: 0.8368\n",
            "Epoch 14/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3900 - accuracy: 0.8382\n",
            "Epoch 15/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3914 - accuracy: 0.8404\n",
            "Epoch 16/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3800 - accuracy: 0.8459\n",
            "Epoch 17/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3911 - accuracy: 0.8371\n",
            "Epoch 18/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3907 - accuracy: 0.8403\n",
            "Epoch 19/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3885 - accuracy: 0.8411\n",
            "Epoch 20/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3895 - accuracy: 0.8413\n",
            "Epoch 21/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3778 - accuracy: 0.8511\n",
            "Epoch 22/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.4043 - accuracy: 0.8329\n",
            "Epoch 23/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3901 - accuracy: 0.8427\n",
            "Epoch 24/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3872 - accuracy: 0.8409\n",
            "Epoch 25/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3882 - accuracy: 0.8407\n",
            "Epoch 26/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.4045 - accuracy: 0.8342\n",
            "Epoch 27/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3742 - accuracy: 0.8530\n",
            "Epoch 28/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3931 - accuracy: 0.8390\n",
            "Epoch 29/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3957 - accuracy: 0.8397\n",
            "Epoch 30/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3766 - accuracy: 0.8497\n",
            "Epoch 31/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3912 - accuracy: 0.8450\n",
            "Epoch 32/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3975 - accuracy: 0.8402\n",
            "Epoch 33/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3808 - accuracy: 0.8466\n",
            "Epoch 34/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3892 - accuracy: 0.8417\n",
            "Epoch 35/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3934 - accuracy: 0.8374\n",
            "Epoch 36/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3934 - accuracy: 0.8371\n",
            "Epoch 37/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3853 - accuracy: 0.8391\n",
            "Epoch 38/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3868 - accuracy: 0.8421\n",
            "Epoch 39/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3801 - accuracy: 0.8470\n",
            "Epoch 40/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3862 - accuracy: 0.8420\n",
            "Epoch 41/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3937 - accuracy: 0.8408\n",
            "Epoch 42/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3971 - accuracy: 0.8348\n",
            "Epoch 43/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3885 - accuracy: 0.8392\n",
            "Epoch 44/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3795 - accuracy: 0.8467\n",
            "Epoch 45/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3962 - accuracy: 0.8363\n",
            "Epoch 46/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3761 - accuracy: 0.8495\n",
            "Epoch 47/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3859 - accuracy: 0.8466\n",
            "Epoch 48/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3823 - accuracy: 0.8423\n",
            "Epoch 49/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3820 - accuracy: 0.8446\n",
            "Epoch 50/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3874 - accuracy: 0.8409\n",
            "Epoch 51/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3994 - accuracy: 0.8337\n",
            "Epoch 52/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3813 - accuracy: 0.8455\n",
            "Epoch 53/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3899 - accuracy: 0.8402\n",
            "Epoch 54/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3877 - accuracy: 0.8402\n",
            "Epoch 55/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3885 - accuracy: 0.8428\n",
            "Epoch 56/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3895 - accuracy: 0.8414\n",
            "Epoch 57/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3956 - accuracy: 0.8355\n",
            "Epoch 58/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3785 - accuracy: 0.8508\n",
            "Epoch 59/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3814 - accuracy: 0.8425\n",
            "Epoch 60/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3855 - accuracy: 0.8452\n",
            "Epoch 61/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3845 - accuracy: 0.8411\n",
            "Epoch 62/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3860 - accuracy: 0.8405\n",
            "Epoch 63/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3837 - accuracy: 0.8427\n",
            "Epoch 64/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3771 - accuracy: 0.8442\n",
            "Epoch 65/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3812 - accuracy: 0.8427\n",
            "Epoch 66/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3802 - accuracy: 0.8452\n",
            "Epoch 67/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3861 - accuracy: 0.8427\n",
            "Epoch 68/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3862 - accuracy: 0.8396\n",
            "Epoch 69/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3860 - accuracy: 0.8427\n",
            "Epoch 70/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3759 - accuracy: 0.8440\n",
            "Epoch 71/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3802 - accuracy: 0.8427\n",
            "Epoch 72/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3968 - accuracy: 0.8360\n",
            "Epoch 73/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3861 - accuracy: 0.8400\n",
            "Epoch 74/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3808 - accuracy: 0.8399\n",
            "Epoch 75/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3696 - accuracy: 0.8501\n",
            "Epoch 76/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3870 - accuracy: 0.8350\n",
            "Epoch 77/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3709 - accuracy: 0.8430\n",
            "Epoch 78/100\n",
            "800/800 [==============================] - 1s 2ms/step - loss: 0.3727 - accuracy: 0.8489\n",
            "Epoch 79/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3622 - accuracy: 0.8542\n",
            "Epoch 80/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3544 - accuracy: 0.8499\n",
            "Epoch 81/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3698 - accuracy: 0.8421\n",
            "Epoch 82/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3459 - accuracy: 0.8575\n",
            "Epoch 83/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3394 - accuracy: 0.8595\n",
            "Epoch 84/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3500 - accuracy: 0.8536\n",
            "Epoch 85/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3366 - accuracy: 0.8656\n",
            "Epoch 86/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3523 - accuracy: 0.8497\n",
            "Epoch 87/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3337 - accuracy: 0.8594\n",
            "Epoch 88/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3313 - accuracy: 0.8688\n",
            "Epoch 89/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3307 - accuracy: 0.8643\n",
            "Epoch 90/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3439 - accuracy: 0.8542\n",
            "Epoch 91/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3391 - accuracy: 0.8607\n",
            "Epoch 92/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3397 - accuracy: 0.8598\n",
            "Epoch 93/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3407 - accuracy: 0.8574\n",
            "Epoch 94/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3291 - accuracy: 0.8674\n",
            "Epoch 95/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3413 - accuracy: 0.8552\n",
            "Epoch 96/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3336 - accuracy: 0.8577\n",
            "Epoch 97/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3309 - accuracy: 0.8682\n",
            "Epoch 98/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3373 - accuracy: 0.8585\n",
            "Epoch 99/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3337 - accuracy: 0.8598\n",
            "Epoch 100/100\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3309 - accuracy: 0.8660\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQ7krMWKOiAv",
        "outputId": "b934dd95-f4ba-41f7-9a44-d10ec2da9318"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8000, 11)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVN2lXr6QAWc",
        "outputId": "53bc8caf-5ae9-48b4-9119-44998c8caad1"
      },
      "source": [
        "loss, accuracy = model.evaluate(X_test, y_test)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "63/63 [==============================] - 0s 1ms/step - loss: 0.3664 - accuracy: 0.8520\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqHmSY5mRMFW",
        "outputId": "f3136665-89a4-4701-9b9f-226f3d5184ac"
      },
      "source": [
        "#predict using the info of a new customer\n",
        "new_customer = [[0,0,600, 1,40,3,60000,2,1,1,50000]]\n",
        "new_customer = sc.transform(sc.transform(new_customer))\n",
        "new_prediction = model.predict(new_customer)\n",
        "new_prediction = (new_prediction >0.5)\n",
        "print(new_prediction)   #means customer will leave the bank"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[False]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "mObuj-82SYve",
        "outputId": "716d210c-c440-4f0a-c6ad-28de6270ac4b"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.show() #plot is not very informative as we ran training several times"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "73YrslspSz_6",
        "outputId": "d53cbb5d-0f98-4ffd-f16c-f1f473e24f18"
      },
      "source": [
        "'''from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X = sc.fit_transform(X)\n",
        "\n",
        "history = model.fit(X,y,verbose=0,\n",
        "                    batch = )\n",
        "'''"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'from sklearn.preprocessing import StandardScaler\\nsc = StandardScaler()\\nX = sc.fit_transform(X)\\n\\nhistory = model.fit(X,y,verbose=0,\\n                    batch = )\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "dNGy8N_YUCBt",
        "outputId": "d8d0b083-b6bb-49d0-a73e-1ae590f677f0"
      },
      "source": [
        "from matplotlib import pyplot as plt \n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.legend(['accuracy', 'validation accuracy', 'loss', 'validation_loss'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel(['epoch'])\n",
        "plt.show()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-80440ed459ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'val_accuracy'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIJq5WBgWjRh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e30a62a-c9fc-47c9-8606-20f6b6c9513f"
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "early_stopping = EarlyStopping(monitor='val_loss',patience=5) \n",
        "#patience = the number of epochs to wait before stopping\n",
        "\n",
        "#we want to save the best model, e.g. 5 echos before it stopped\n",
        "callbacks = [early_stopping, ModelCheckpoint(filepath='best_model.h5', \n",
        "                                             monitor= 'val_loss',\n",
        "                                             save_best_only=True)] \n",
        "history = model.fit(X_train, \n",
        "                    y_train, \n",
        "                    epochs=200, \n",
        "                    validation_split=0.25, \n",
        "                    batch_size=40, \n",
        "                    verbose=2, \n",
        "                    callbacks = [callbacks])\u001d"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "150/150 - 1s - loss: 0.3323 - accuracy: 0.8603 - val_loss: 0.3293 - val_accuracy: 0.8620\n",
            "Epoch 2/200\n",
            "150/150 - 0s - loss: 0.3313 - accuracy: 0.8640 - val_loss: 0.3298 - val_accuracy: 0.8630\n",
            "Epoch 3/200\n",
            "150/150 - 0s - loss: 0.3313 - accuracy: 0.8628 - val_loss: 0.3307 - val_accuracy: 0.8635\n",
            "Epoch 4/200\n",
            "150/150 - 0s - loss: 0.3310 - accuracy: 0.8633 - val_loss: 0.3301 - val_accuracy: 0.8620\n",
            "Epoch 5/200\n",
            "150/150 - 0s - loss: 0.3311 - accuracy: 0.8637 - val_loss: 0.3312 - val_accuracy: 0.8640\n",
            "Epoch 6/200\n",
            "150/150 - 0s - loss: 0.3311 - accuracy: 0.8623 - val_loss: 0.3317 - val_accuracy: 0.8630\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sq1o41Mz5jGK",
        "outputId": "0513fc7c-c7da-403e-b6bd-5d2f2174acb3"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "n_split = 20\n",
        "def create_model():\n",
        "  model = Sequential()\n",
        "  model.add(Dense(units=10, input_shape=(11, ), kernel_initializer = 'uniform', activation = 'relu', name='dense_layer1'))\n",
        "  model.add(Dense(units=20, input_shape=(11, ), kernel_initializer = 'uniform', activation = 'relu', name='dense_layer2'))\n",
        "  model.add(Dense(units=30, input_shape=(11, ),kernel_initializer = 'uniform',activation = 'relu', name='dense_layer3'))\n",
        "  model.add(Dense(units=50, input_shape=(11, ),kernel_initializer = 'uniform',activation = 'relu', name='dense_layer4'))\n",
        "  model.add(Dense(1, activation = 'sigmoid', name = 'dense_output'))\n",
        "  model.compile(loss='binary_crossentropy',   #binary_crossentropy\n",
        "              optimizer= 'adam',\n",
        "              metrics = ['accuracy']) #we want to use adam optimizer\n",
        "  return model\n",
        "\n",
        "for train_index, test_index in KFold(n_split).split(X):\n",
        "  x_train, x_test = X[train_index], X[test_index]\n",
        "  y_train, y_test = y[train_index], y[test_index]\n",
        "  model = create_model()\n",
        "  model.fit(x_train, y_train, epochs=20)\n",
        "\n",
        "  print('Model evaluation ', model.evaluate(x_test, y_test))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "297/297 [==============================] - 1s 1ms/step - loss: 0.5440 - accuracy: 0.7950\n",
            "Epoch 2/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5060 - accuracy: 0.8012\n",
            "Epoch 3/20\n",
            "297/297 [==============================] - 0s 1ms/step - loss: 0.5065 - accuracy: 0.7941\n",
            "Epoch 4/20\n",
            "297/297 [==============================] - 0s 1ms/step - loss: 0.5092 - accuracy: 0.7906\n",
            "Epoch 5/20\n",
            "297/297 [==============================] - 0s 1ms/step - loss: 0.4979 - accuracy: 0.7971\n",
            "Epoch 6/20\n",
            "297/297 [==============================] - 0s 1ms/step - loss: 0.5024 - accuracy: 0.7931\n",
            "Epoch 7/20\n",
            "297/297 [==============================] - 0s 1ms/step - loss: 0.4907 - accuracy: 0.8023\n",
            "Epoch 8/20\n",
            "297/297 [==============================] - 0s 1ms/step - loss: 0.5010 - accuracy: 0.7954\n",
            "Epoch 9/20\n",
            "297/297 [==============================] - 0s 1ms/step - loss: 0.5021 - accuracy: 0.7929\n",
            "Epoch 10/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5050 - accuracy: 0.7922\n",
            "Epoch 11/20\n",
            "297/297 [==============================] - 0s 1ms/step - loss: 0.4998 - accuracy: 0.7959\n",
            "Epoch 12/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5002 - accuracy: 0.7950\n",
            "Epoch 13/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4972 - accuracy: 0.7968\n",
            "Epoch 14/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4995 - accuracy: 0.7955\n",
            "Epoch 15/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4947 - accuracy: 0.8006\n",
            "Epoch 16/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5059 - accuracy: 0.7932\n",
            "Epoch 17/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5004 - accuracy: 0.7921\n",
            "Epoch 18/20\n",
            "297/297 [==============================] - 0s 1ms/step - loss: 0.5032 - accuracy: 0.7926\n",
            "Epoch 19/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5027 - accuracy: 0.7935\n",
            "Epoch 20/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4930 - accuracy: 0.8005\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.5048 - accuracy: 0.7960\n",
            "Model evaluation  [0.5048184990882874, 0.7960000038146973]\n",
            "Epoch 1/20\n",
            "297/297 [==============================] - 1s 1ms/step - loss: 0.5462 - accuracy: 0.7855\n",
            "Epoch 2/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5048 - accuracy: 0.8015\n",
            "Epoch 3/20\n",
            "297/297 [==============================] - 0s 1ms/step - loss: 0.4993 - accuracy: 0.7985\n",
            "Epoch 4/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4988 - accuracy: 0.7964\n",
            "Epoch 5/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4997 - accuracy: 0.7969\n",
            "Epoch 6/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4910 - accuracy: 0.8034\n",
            "Epoch 7/20\n",
            "297/297 [==============================] - 0s 1ms/step - loss: 0.5040 - accuracy: 0.7922\n",
            "Epoch 8/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4997 - accuracy: 0.7968\n",
            "Epoch 9/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5114 - accuracy: 0.7876\n",
            "Epoch 10/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5018 - accuracy: 0.7943\n",
            "Epoch 11/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4886 - accuracy: 0.8037\n",
            "Epoch 12/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4939 - accuracy: 0.8001\n",
            "Epoch 13/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4874 - accuracy: 0.8041\n",
            "Epoch 14/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5040 - accuracy: 0.7929\n",
            "Epoch 15/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4989 - accuracy: 0.7970\n",
            "Epoch 16/20\n",
            "297/297 [==============================] - 0s 1ms/step - loss: 0.5069 - accuracy: 0.7897\n",
            "Epoch 17/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4875 - accuracy: 0.8039\n",
            "Epoch 18/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4920 - accuracy: 0.8004\n",
            "Epoch 19/20\n",
            "297/297 [==============================] - 0s 1ms/step - loss: 0.4951 - accuracy: 0.7962\n",
            "Epoch 20/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5046 - accuracy: 0.7921\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.5045 - accuracy: 0.7960\n",
            "Model evaluation  [0.5044745802879333, 0.7960000038146973]\n",
            "Epoch 1/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5593 - accuracy: 0.7916\n",
            "Epoch 2/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5172 - accuracy: 0.7945\n",
            "Epoch 3/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5010 - accuracy: 0.7982\n",
            "Epoch 4/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4975 - accuracy: 0.7971\n",
            "Epoch 5/20\n",
            "297/297 [==============================] - 0s 1ms/step - loss: 0.5025 - accuracy: 0.7964\n",
            "Epoch 6/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5004 - accuracy: 0.7958\n",
            "Epoch 7/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5010 - accuracy: 0.7931\n",
            "Epoch 8/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4967 - accuracy: 0.7987\n",
            "Epoch 9/20\n",
            "297/297 [==============================] - 0s 1ms/step - loss: 0.4963 - accuracy: 0.7985\n",
            "Epoch 10/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5014 - accuracy: 0.7955\n",
            "Epoch 11/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5136 - accuracy: 0.7852\n",
            "Epoch 12/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5111 - accuracy: 0.7877\n",
            "Epoch 13/20\n",
            "297/297 [==============================] - 0s 1ms/step - loss: 0.5013 - accuracy: 0.7962\n",
            "Epoch 14/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5115 - accuracy: 0.7920\n",
            "Epoch 15/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5000 - accuracy: 0.8001\n",
            "Epoch 16/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5090 - accuracy: 0.7932\n",
            "Epoch 17/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5058 - accuracy: 0.7960\n",
            "Epoch 18/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4948 - accuracy: 0.8041\n",
            "Epoch 19/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5031 - accuracy: 0.7956\n",
            "Epoch 20/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5075 - accuracy: 0.7950\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.5636 - accuracy: 0.7640\n",
            "Model evaluation  [0.5635709762573242, 0.7639999985694885]\n",
            "Epoch 1/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5491 - accuracy: 0.7862\n",
            "Epoch 2/20\n",
            "297/297 [==============================] - 0s 1ms/step - loss: 0.5245 - accuracy: 0.7909\n",
            "Epoch 3/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5081 - accuracy: 0.7937\n",
            "Epoch 4/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5017 - accuracy: 0.7933\n",
            "Epoch 5/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5052 - accuracy: 0.7917\n",
            "Epoch 6/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4952 - accuracy: 0.8003\n",
            "Epoch 7/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5065 - accuracy: 0.7918\n",
            "Epoch 8/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5025 - accuracy: 0.7934\n",
            "Epoch 9/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5022 - accuracy: 0.7947\n",
            "Epoch 10/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5079 - accuracy: 0.7902\n",
            "Epoch 11/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4981 - accuracy: 0.7966\n",
            "Epoch 12/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4936 - accuracy: 0.7987\n",
            "Epoch 13/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5011 - accuracy: 0.7949\n",
            "Epoch 14/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4945 - accuracy: 0.8011\n",
            "Epoch 15/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5072 - accuracy: 0.7925\n",
            "Epoch 16/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5073 - accuracy: 0.7901\n",
            "Epoch 17/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4964 - accuracy: 0.7979\n",
            "Epoch 18/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4906 - accuracy: 0.8008\n",
            "Epoch 19/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5069 - accuracy: 0.7909\n",
            "Epoch 20/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4958 - accuracy: 0.7975\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.4717 - accuracy: 0.8120\n",
            "Model evaluation  [0.471659392118454, 0.8119999766349792]\n",
            "Epoch 1/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5329 - accuracy: 0.7848\n",
            "Epoch 2/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5067 - accuracy: 0.7971\n",
            "Epoch 3/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4969 - accuracy: 0.7996\n",
            "Epoch 4/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4986 - accuracy: 0.7960\n",
            "Epoch 5/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5022 - accuracy: 0.7961\n",
            "Epoch 6/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5025 - accuracy: 0.7952\n",
            "Epoch 7/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4927 - accuracy: 0.8010\n",
            "Epoch 8/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5048 - accuracy: 0.7923\n",
            "Epoch 9/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4959 - accuracy: 0.7989\n",
            "Epoch 10/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5026 - accuracy: 0.7934\n",
            "Epoch 11/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5021 - accuracy: 0.7937\n",
            "Epoch 12/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5008 - accuracy: 0.7947\n",
            "Epoch 13/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4965 - accuracy: 0.7990\n",
            "Epoch 14/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4971 - accuracy: 0.7977\n",
            "Epoch 15/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5002 - accuracy: 0.7947\n",
            "Epoch 16/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4944 - accuracy: 0.7992\n",
            "Epoch 17/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5004 - accuracy: 0.7962\n",
            "Epoch 18/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5000 - accuracy: 0.7988\n",
            "Epoch 19/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4970 - accuracy: 0.8003\n",
            "Epoch 20/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4940 - accuracy: 0.7995\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.5275 - accuracy: 0.7720\n",
            "Model evaluation  [0.527526319026947, 0.7720000147819519]\n",
            "Epoch 1/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.6203 - accuracy: 0.7766\n",
            "Epoch 2/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5125 - accuracy: 0.7981\n",
            "Epoch 3/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4980 - accuracy: 0.8011\n",
            "Epoch 4/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5087 - accuracy: 0.7913\n",
            "Epoch 5/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4924 - accuracy: 0.8000\n",
            "Epoch 6/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5009 - accuracy: 0.7954\n",
            "Epoch 7/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4940 - accuracy: 0.7976\n",
            "Epoch 8/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5147 - accuracy: 0.7864\n",
            "Epoch 9/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5006 - accuracy: 0.7946\n",
            "Epoch 10/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4861 - accuracy: 0.8060\n",
            "Epoch 11/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4894 - accuracy: 0.8027\n",
            "Epoch 12/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4951 - accuracy: 0.8015\n",
            "Epoch 13/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4987 - accuracy: 0.7955\n",
            "Epoch 14/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4983 - accuracy: 0.7984\n",
            "Epoch 15/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5063 - accuracy: 0.7900\n",
            "Epoch 16/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5033 - accuracy: 0.7940\n",
            "Epoch 17/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4974 - accuracy: 0.7967\n",
            "Epoch 18/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4948 - accuracy: 0.7987\n",
            "Epoch 19/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5032 - accuracy: 0.7938\n",
            "Epoch 20/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5011 - accuracy: 0.7953\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.5055 - accuracy: 0.7960\n",
            "Model evaluation  [0.5055071711540222, 0.7960000038146973]\n",
            "Epoch 1/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.6162 - accuracy: 0.7655\n",
            "Epoch 2/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5118 - accuracy: 0.7983\n",
            "Epoch 3/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5104 - accuracy: 0.7947\n",
            "Epoch 4/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5015 - accuracy: 0.7984\n",
            "Epoch 5/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4931 - accuracy: 0.8009\n",
            "Epoch 6/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5036 - accuracy: 0.7935\n",
            "Epoch 7/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4984 - accuracy: 0.7985\n",
            "Epoch 8/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4983 - accuracy: 0.7969\n",
            "Epoch 9/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4841 - accuracy: 0.8075\n",
            "Epoch 10/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5101 - accuracy: 0.7893\n",
            "Epoch 11/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5087 - accuracy: 0.7917\n",
            "Epoch 12/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5097 - accuracy: 0.7886\n",
            "Epoch 13/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5039 - accuracy: 0.7955\n",
            "Epoch 14/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4999 - accuracy: 0.7961\n",
            "Epoch 15/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4995 - accuracy: 0.7957\n",
            "Epoch 16/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4903 - accuracy: 0.8009\n",
            "Epoch 17/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4983 - accuracy: 0.7977\n",
            "Epoch 18/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5022 - accuracy: 0.7926\n",
            "Epoch 19/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4976 - accuracy: 0.7966\n",
            "Epoch 20/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4996 - accuracy: 0.7939\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.5167 - accuracy: 0.7880\n",
            "Model evaluation  [0.5167309641838074, 0.7879999876022339]\n",
            "Epoch 1/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5392 - accuracy: 0.7872\n",
            "Epoch 2/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5154 - accuracy: 0.7920\n",
            "Epoch 3/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5105 - accuracy: 0.7881\n",
            "Epoch 4/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4972 - accuracy: 0.7993\n",
            "Epoch 5/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5069 - accuracy: 0.7904\n",
            "Epoch 6/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5087 - accuracy: 0.7901\n",
            "Epoch 7/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4980 - accuracy: 0.7979\n",
            "Epoch 8/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5032 - accuracy: 0.7945\n",
            "Epoch 9/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4951 - accuracy: 0.8009\n",
            "Epoch 10/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4895 - accuracy: 0.8050\n",
            "Epoch 11/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5022 - accuracy: 0.7948\n",
            "Epoch 12/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5012 - accuracy: 0.7965\n",
            "Epoch 13/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4928 - accuracy: 0.8011\n",
            "Epoch 14/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4882 - accuracy: 0.8034\n",
            "Epoch 15/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4914 - accuracy: 0.7990\n",
            "Epoch 16/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5001 - accuracy: 0.7981\n",
            "Epoch 17/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4980 - accuracy: 0.7998\n",
            "Epoch 18/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4979 - accuracy: 0.7985\n",
            "Epoch 19/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4998 - accuracy: 0.7947\n",
            "Epoch 20/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5042 - accuracy: 0.7932\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.5181 - accuracy: 0.7840\n",
            "Model evaluation  [0.5181394815444946, 0.7839999794960022]\n",
            "Epoch 1/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5819 - accuracy: 0.7767\n",
            "Epoch 2/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5119 - accuracy: 0.8010\n",
            "Epoch 3/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5067 - accuracy: 0.7955\n",
            "Epoch 4/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5058 - accuracy: 0.7962\n",
            "Epoch 5/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4927 - accuracy: 0.8019\n",
            "Epoch 6/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5064 - accuracy: 0.7912\n",
            "Epoch 7/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4976 - accuracy: 0.7970\n",
            "Epoch 8/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4981 - accuracy: 0.7971\n",
            "Epoch 9/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4951 - accuracy: 0.7996\n",
            "Epoch 10/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5039 - accuracy: 0.7935\n",
            "Epoch 11/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4932 - accuracy: 0.8001\n",
            "Epoch 12/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4959 - accuracy: 0.7995\n",
            "Epoch 13/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5241 - accuracy: 0.7792\n",
            "Epoch 14/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5088 - accuracy: 0.7894\n",
            "Epoch 15/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4996 - accuracy: 0.7948\n",
            "Epoch 16/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4968 - accuracy: 0.7977\n",
            "Epoch 17/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5034 - accuracy: 0.7942\n",
            "Epoch 18/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5104 - accuracy: 0.7865\n",
            "Epoch 19/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5011 - accuracy: 0.7951\n",
            "Epoch 20/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4909 - accuracy: 0.8013\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.5016 - accuracy: 0.7980\n",
            "Model evaluation  [0.5016482472419739, 0.7979999780654907]\n",
            "Epoch 1/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5925 - accuracy: 0.7832\n",
            "Epoch 2/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5211 - accuracy: 0.7975\n",
            "Epoch 3/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5068 - accuracy: 0.7949\n",
            "Epoch 4/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4982 - accuracy: 0.7989\n",
            "Epoch 5/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5009 - accuracy: 0.7965\n",
            "Epoch 6/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5016 - accuracy: 0.7954\n",
            "Epoch 7/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5063 - accuracy: 0.7936\n",
            "Epoch 8/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5009 - accuracy: 0.7938\n",
            "Epoch 9/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4984 - accuracy: 0.7979\n",
            "Epoch 10/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5070 - accuracy: 0.7918\n",
            "Epoch 11/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4995 - accuracy: 0.7957\n",
            "Epoch 12/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5043 - accuracy: 0.7907\n",
            "Epoch 13/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4923 - accuracy: 0.8027\n",
            "Epoch 14/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4834 - accuracy: 0.8067\n",
            "Epoch 15/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4940 - accuracy: 0.8006\n",
            "Epoch 16/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5059 - accuracy: 0.7914\n",
            "Epoch 17/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5115 - accuracy: 0.7891\n",
            "Epoch 18/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5080 - accuracy: 0.7893\n",
            "Epoch 19/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4989 - accuracy: 0.7960\n",
            "Epoch 20/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4944 - accuracy: 0.7978\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.4922 - accuracy: 0.8020\n",
            "Model evaluation  [0.4921850562095642, 0.8019999861717224]\n",
            "Epoch 1/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5452 - accuracy: 0.7904\n",
            "Epoch 2/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5218 - accuracy: 0.7971\n",
            "Epoch 3/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5020 - accuracy: 0.8009\n",
            "Epoch 4/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5068 - accuracy: 0.7945\n",
            "Epoch 5/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5004 - accuracy: 0.7960\n",
            "Epoch 6/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5007 - accuracy: 0.7966\n",
            "Epoch 7/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5110 - accuracy: 0.7875\n",
            "Epoch 8/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5026 - accuracy: 0.7934\n",
            "Epoch 9/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5023 - accuracy: 0.7936\n",
            "Epoch 10/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5059 - accuracy: 0.7918\n",
            "Epoch 11/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4948 - accuracy: 0.7991\n",
            "Epoch 12/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4853 - accuracy: 0.8066\n",
            "Epoch 13/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5002 - accuracy: 0.7961\n",
            "Epoch 14/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4908 - accuracy: 0.8016\n",
            "Epoch 15/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4969 - accuracy: 0.7982\n",
            "Epoch 16/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5021 - accuracy: 0.7944\n",
            "Epoch 17/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4939 - accuracy: 0.7999\n",
            "Epoch 18/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5011 - accuracy: 0.7929\n",
            "Epoch 19/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5020 - accuracy: 0.7939\n",
            "Epoch 20/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5011 - accuracy: 0.7942\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.4993 - accuracy: 0.7900\n",
            "Model evaluation  [0.4993340075016022, 0.7900000214576721]\n",
            "Epoch 1/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5862 - accuracy: 0.7750\n",
            "Epoch 2/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5032 - accuracy: 0.8032\n",
            "Epoch 3/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5140 - accuracy: 0.7930\n",
            "Epoch 4/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5095 - accuracy: 0.7921\n",
            "Epoch 5/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4998 - accuracy: 0.7985\n",
            "Epoch 6/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5027 - accuracy: 0.7934\n",
            "Epoch 7/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4992 - accuracy: 0.7964\n",
            "Epoch 8/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5043 - accuracy: 0.7921\n",
            "Epoch 9/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5019 - accuracy: 0.7953\n",
            "Epoch 10/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4974 - accuracy: 0.7992\n",
            "Epoch 11/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4988 - accuracy: 0.7964\n",
            "Epoch 12/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5021 - accuracy: 0.7945\n",
            "Epoch 13/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5017 - accuracy: 0.7933\n",
            "Epoch 14/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5023 - accuracy: 0.7972\n",
            "Epoch 15/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5011 - accuracy: 0.7944\n",
            "Epoch 16/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4898 - accuracy: 0.8029\n",
            "Epoch 17/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4940 - accuracy: 0.8001\n",
            "Epoch 18/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4999 - accuracy: 0.7945\n",
            "Epoch 19/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4939 - accuracy: 0.7987\n",
            "Epoch 20/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4942 - accuracy: 0.8000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.4953 - accuracy: 0.8040\n",
            "Model evaluation  [0.49528539180755615, 0.8040000200271606]\n",
            "Epoch 1/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.6425 - accuracy: 0.7824\n",
            "Epoch 2/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5189 - accuracy: 0.7963\n",
            "Epoch 3/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5099 - accuracy: 0.7928\n",
            "Epoch 4/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5004 - accuracy: 0.7970\n",
            "Epoch 5/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4926 - accuracy: 0.8011\n",
            "Epoch 6/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4978 - accuracy: 0.7970\n",
            "Epoch 7/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5097 - accuracy: 0.7892\n",
            "Epoch 8/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4896 - accuracy: 0.8025\n",
            "Epoch 9/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5043 - accuracy: 0.7922\n",
            "Epoch 10/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4919 - accuracy: 0.8004\n",
            "Epoch 11/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5034 - accuracy: 0.7927\n",
            "Epoch 12/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5065 - accuracy: 0.7906\n",
            "Epoch 13/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5024 - accuracy: 0.7931\n",
            "Epoch 14/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5017 - accuracy: 0.7948\n",
            "Epoch 15/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4998 - accuracy: 0.7961\n",
            "Epoch 16/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4991 - accuracy: 0.7957\n",
            "Epoch 17/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5115 - accuracy: 0.7880\n",
            "Epoch 18/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5013 - accuracy: 0.7989\n",
            "Epoch 19/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5030 - accuracy: 0.7978\n",
            "Epoch 20/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5018 - accuracy: 0.7945\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.4851 - accuracy: 0.8040\n",
            "Model evaluation  [0.48509275913238525, 0.8040000200271606]\n",
            "Epoch 1/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5413 - accuracy: 0.7964\n",
            "Epoch 2/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5205 - accuracy: 0.7941\n",
            "Epoch 3/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5128 - accuracy: 0.7942\n",
            "Epoch 4/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5089 - accuracy: 0.7903\n",
            "Epoch 5/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5050 - accuracy: 0.7930\n",
            "Epoch 6/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5052 - accuracy: 0.7926\n",
            "Epoch 7/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5087 - accuracy: 0.7926\n",
            "Epoch 8/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4941 - accuracy: 0.7992\n",
            "Epoch 9/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5040 - accuracy: 0.7913\n",
            "Epoch 10/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5024 - accuracy: 0.7945\n",
            "Epoch 11/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5031 - accuracy: 0.7948\n",
            "Epoch 12/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5021 - accuracy: 0.7918\n",
            "Epoch 13/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4994 - accuracy: 0.7950\n",
            "Epoch 14/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4961 - accuracy: 0.7980\n",
            "Epoch 15/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4995 - accuracy: 0.7961\n",
            "Epoch 16/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5007 - accuracy: 0.7942\n",
            "Epoch 17/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4986 - accuracy: 0.7955\n",
            "Epoch 18/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4939 - accuracy: 0.7994\n",
            "Epoch 19/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4957 - accuracy: 0.7968\n",
            "Epoch 20/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4961 - accuracy: 0.7955\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.4885 - accuracy: 0.8080\n",
            "Model evaluation  [0.4884607195854187, 0.8080000281333923]\n",
            "Epoch 1/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.6034 - accuracy: 0.7653\n",
            "Epoch 2/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5217 - accuracy: 0.7939\n",
            "Epoch 3/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5091 - accuracy: 0.7948\n",
            "Epoch 4/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4975 - accuracy: 0.7994\n",
            "Epoch 5/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5050 - accuracy: 0.7920\n",
            "Epoch 6/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5003 - accuracy: 0.7970\n",
            "Epoch 7/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4895 - accuracy: 0.8029\n",
            "Epoch 8/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5027 - accuracy: 0.7944\n",
            "Epoch 9/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4931 - accuracy: 0.8000\n",
            "Epoch 10/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5021 - accuracy: 0.7956\n",
            "Epoch 11/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5002 - accuracy: 0.7957\n",
            "Epoch 12/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5006 - accuracy: 0.7959\n",
            "Epoch 13/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5122 - accuracy: 0.7866\n",
            "Epoch 14/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5057 - accuracy: 0.7945\n",
            "Epoch 15/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4929 - accuracy: 0.8046\n",
            "Epoch 16/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4977 - accuracy: 0.7979\n",
            "Epoch 17/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5089 - accuracy: 0.7894\n",
            "Epoch 18/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4858 - accuracy: 0.8071\n",
            "Epoch 19/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4926 - accuracy: 0.8006\n",
            "Epoch 20/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4895 - accuracy: 0.8040\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.4992 - accuracy: 0.7940\n",
            "Model evaluation  [0.4992028772830963, 0.7940000295639038]\n",
            "Epoch 1/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5440 - accuracy: 0.7893\n",
            "Epoch 2/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5159 - accuracy: 0.7928\n",
            "Epoch 3/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5087 - accuracy: 0.7912\n",
            "Epoch 4/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5081 - accuracy: 0.7932\n",
            "Epoch 5/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4900 - accuracy: 0.8038\n",
            "Epoch 6/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5012 - accuracy: 0.7950\n",
            "Epoch 7/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4980 - accuracy: 0.7967\n",
            "Epoch 8/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4935 - accuracy: 0.8007\n",
            "Epoch 9/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5054 - accuracy: 0.7930\n",
            "Epoch 10/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4902 - accuracy: 0.8038\n",
            "Epoch 11/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4978 - accuracy: 0.7980\n",
            "Epoch 12/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5029 - accuracy: 0.7943\n",
            "Epoch 13/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4932 - accuracy: 0.8020\n",
            "Epoch 14/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4954 - accuracy: 0.8030\n",
            "Epoch 15/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4997 - accuracy: 0.7982\n",
            "Epoch 16/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5001 - accuracy: 0.7950\n",
            "Epoch 17/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5007 - accuracy: 0.7930\n",
            "Epoch 18/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5087 - accuracy: 0.7913\n",
            "Epoch 19/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4935 - accuracy: 0.8009\n",
            "Epoch 20/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4993 - accuracy: 0.7979\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.4947 - accuracy: 0.7980\n",
            "Model evaluation  [0.4947313964366913, 0.7979999780654907]\n",
            "Epoch 1/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5590 - accuracy: 0.7897\n",
            "Epoch 2/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5130 - accuracy: 0.7992\n",
            "Epoch 3/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5018 - accuracy: 0.7999\n",
            "Epoch 4/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4997 - accuracy: 0.7976\n",
            "Epoch 5/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4912 - accuracy: 0.8022\n",
            "Epoch 6/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5015 - accuracy: 0.7942\n",
            "Epoch 7/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5015 - accuracy: 0.7955\n",
            "Epoch 8/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4921 - accuracy: 0.7996\n",
            "Epoch 9/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5016 - accuracy: 0.7956\n",
            "Epoch 10/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4956 - accuracy: 0.7988\n",
            "Epoch 11/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4897 - accuracy: 0.8033\n",
            "Epoch 12/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4949 - accuracy: 0.8003\n",
            "Epoch 13/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4906 - accuracy: 0.8028\n",
            "Epoch 14/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4977 - accuracy: 0.7962\n",
            "Epoch 15/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4957 - accuracy: 0.7986\n",
            "Epoch 16/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4981 - accuracy: 0.7970\n",
            "Epoch 17/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4932 - accuracy: 0.8011\n",
            "Epoch 18/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4935 - accuracy: 0.8025\n",
            "Epoch 19/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5067 - accuracy: 0.7914\n",
            "Epoch 20/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4951 - accuracy: 0.7989\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.5381 - accuracy: 0.7660\n",
            "Model evaluation  [0.5380866527557373, 0.765999972820282]\n",
            "Epoch 1/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5457 - accuracy: 0.7848\n",
            "Epoch 2/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5218 - accuracy: 0.7871\n",
            "Epoch 3/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5118 - accuracy: 0.7917\n",
            "Epoch 4/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4993 - accuracy: 0.7968\n",
            "Epoch 5/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4935 - accuracy: 0.8008\n",
            "Epoch 6/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4962 - accuracy: 0.7997\n",
            "Epoch 7/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5011 - accuracy: 0.7960\n",
            "Epoch 8/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4972 - accuracy: 0.7983\n",
            "Epoch 9/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4998 - accuracy: 0.7956\n",
            "Epoch 10/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5084 - accuracy: 0.7894\n",
            "Epoch 11/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4985 - accuracy: 0.7985\n",
            "Epoch 12/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4934 - accuracy: 0.7994\n",
            "Epoch 13/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5044 - accuracy: 0.7919\n",
            "Epoch 14/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5040 - accuracy: 0.7961\n",
            "Epoch 15/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5038 - accuracy: 0.7947\n",
            "Epoch 16/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4992 - accuracy: 0.7961\n",
            "Epoch 17/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4983 - accuracy: 0.7966\n",
            "Epoch 18/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5031 - accuracy: 0.7928\n",
            "Epoch 19/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5027 - accuracy: 0.7927\n",
            "Epoch 20/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5117 - accuracy: 0.7855\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.4727 - accuracy: 0.8120\n",
            "Model evaluation  [0.47266295552253723, 0.8119999766349792]\n",
            "Epoch 1/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5697 - accuracy: 0.7762\n",
            "Epoch 2/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5242 - accuracy: 0.7901\n",
            "Epoch 3/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5086 - accuracy: 0.7930\n",
            "Epoch 4/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5110 - accuracy: 0.7894\n",
            "Epoch 5/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5000 - accuracy: 0.7964\n",
            "Epoch 6/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.4965 - accuracy: 0.7995\n",
            "Epoch 7/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5040 - accuracy: 0.7924\n",
            "Epoch 8/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5041 - accuracy: 0.7933\n",
            "Epoch 9/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5015 - accuracy: 0.7952\n",
            "Epoch 10/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5000 - accuracy: 0.7962\n",
            "Epoch 11/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5059 - accuracy: 0.7903\n",
            "Epoch 12/20\n",
            "297/297 [==============================] - 0s 2ms/step - loss: 0.5017 - accuracy: 0.7941\n",
            "Epoch 13/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5031 - accuracy: 0.7922\n",
            "Epoch 14/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5030 - accuracy: 0.7935\n",
            "Epoch 15/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5050 - accuracy: 0.7909\n",
            "Epoch 16/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4999 - accuracy: 0.7929\n",
            "Epoch 17/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4978 - accuracy: 0.7963\n",
            "Epoch 18/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5000 - accuracy: 0.7946\n",
            "Epoch 19/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4984 - accuracy: 0.7963\n",
            "Epoch 20/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4929 - accuracy: 0.7999\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.4766 - accuracy: 0.8160\n",
            "Model evaluation  [0.4765930771827698, 0.8159999847412109]\n",
            "Epoch 1/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5602 - accuracy: 0.7895\n",
            "Epoch 2/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5156 - accuracy: 0.7959\n",
            "Epoch 3/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5131 - accuracy: 0.7894\n",
            "Epoch 4/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4962 - accuracy: 0.7997\n",
            "Epoch 5/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5059 - accuracy: 0.7924\n",
            "Epoch 6/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5081 - accuracy: 0.7905\n",
            "Epoch 7/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4942 - accuracy: 0.7991\n",
            "Epoch 8/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5094 - accuracy: 0.7893\n",
            "Epoch 9/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5006 - accuracy: 0.7957\n",
            "Epoch 10/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5000 - accuracy: 0.7943\n",
            "Epoch 11/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4999 - accuracy: 0.7963\n",
            "Epoch 12/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5064 - accuracy: 0.7905\n",
            "Epoch 13/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5050 - accuracy: 0.7933\n",
            "Epoch 14/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4917 - accuracy: 0.8008\n",
            "Epoch 15/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5007 - accuracy: 0.7954\n",
            "Epoch 16/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5006 - accuracy: 0.7962\n",
            "Epoch 17/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4943 - accuracy: 0.7995\n",
            "Epoch 18/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4973 - accuracy: 0.7970\n",
            "Epoch 19/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.4917 - accuracy: 0.8018\n",
            "Epoch 20/20\n",
            "297/297 [==============================] - 1s 2ms/step - loss: 0.5042 - accuracy: 0.7937\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.4496 - accuracy: 0.8260\n",
            "Model evaluation  [0.4495536684989929, 0.8259999752044678]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VclkG8pB6Mx5",
        "outputId": "8e1520f0-1b5d-477c-bef2-c2eefe818ebf"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOARf6dfDS6h"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}